find_rtools()
library(devtools)
library(devtools);
find.package("devtools")
find.package("devtools");
install.packages("devtools")
library(devtools)
find_rtools()
install.package("KernSmooth")
install.packages("KernSmooth")
import(KernSmooth)
imports(KernSmooth)
suppressMessages(require(KernSmooth))
import(KernSmooth)
add <- function(x,y) {
x + y
}
add(3,5)
above <- function(x,n) {
use <- x > n
x[use]
}
x <- 1:20
above(2)
above(x)
above(x,12)
above <- function(x,n = 10) {
use <- x > n
x[use]
}
above(x)
columnmean <- function(x) {
nc <- ncol(x)
means <- numeric(nc)
for(i in 1:nc) {
means[i] <- mean(x[,i])
}
means
}
columnmean(airquality)
columnmean <- function(x,removeNA = TRUE) {
nc <- ncol(x)
means <- numeric(nc)
for(i in 1:nc) {
means[i] <- mean(x[,i], na.rm = removeNA)
}
means
}
columnmean(airquality)
?conf.int
?confint
confint(data4.lm,level = .98)
confint(data4.lm,level = .98)
E
EEEEE
EE
E
v v v v v
n n n n nb n nb nb n
b bn nb n n n bhjb jh <- njb
bbbbhbhbhbhbbbbbbbbbbbb
vgvhv
bhhbhbhbhbhbh
hbkh
j j j j bbbbbb
njnj
hjh
require(igraph)
library(Rgraphviz)
source("http://www.bioconductor.org/biocLite.R")
biocLite("Ruuid")
biocLite("graph")Replace broken link
biocLite("graph")
biocLite("Rgraphviz")
library(Rgraphviz)
string <- HABBIEIHIHAAHAEBGCHBBHICDHGCHIHBBGBBBBGGIGACHABHACIBCBGIIBBAIBAIABBBAIGEBBCBCBCBDACBBBBFAAGAAAAAFAFF
string <- "HABBIEIHIHAAHAEBGCHBBHICDHGCHIHBBGBBBBGGIGACHABHACIBCBGIIBBAIBAIABBBAIGEBBCBCBCBDACBBBBFAAGAAAAAFAFF"
?str_count()
library(stringr)
?str_count()
str_count(string,'A')
str_count(string,'B')
str_count(string,'C')
str_count(string,'D')
str_count(string,'E')
str_count(string,'F')
str_count(string,'G')
str_count(string,'H')
str_count(string,'I')
str_count(string,'J')
str_count(string,'K')
str_count(string,'L')
str_count(string,'M')
str_count(string,'N')
str_count(string,'O')
str_count(string,'P')
str_count(string,'Q')
str_count(string,'R')
str_count(string,'S')
str_count(string,'T')
str_count(string,'U')
str_count(string,'V')
str_count(string,'W')
str_count(string,'X')
str_count(string,'Y')
str_count(string,'Z')
str_count(string)
count(string)
str_count(string,)
str_count(string, ' ')
n <- 011110011100111110101110011011011111110
n
n <- c(011110011100111110101110011011011111110 )
n
library(bestglm)
install.packages(bestglm)
install.packages("bestglm")
library(bestglm)
library(Boston)
library(MASS)
Boston
set.seed(1)
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
training
summary(Boston)
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
training
training
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xybind <- cbind(X,y)
Xybind
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston <- bestglm(Xybind,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston <- bestglm(Xybind,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
training
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
?Boston
?bestglm
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 3, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
set.seed(1)
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 6, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
library(glmnet)
X <- model.matrix(crim ~ ., Boston)
X
X <- model.matrix(crim ~ ., Boston)[,-1]
X
y <- Boston$crim
y
X <- model.matrix(crim ~ ., Boston)[,-1]
y <- Boston$crim
X <- model.matrix(crim ~ ., Boston)[,-1]
y <- Boston$crim
set.seed(1)
nfolds <- nrow(Boston)
glmnet.ridge.fit <- cv.glmnet(X[training,],y[training],type.measure = "mse",family = "gaussian", alpha = 0, standardize = TRUE)
glmnet.ridge.fit
plot(glmnet.ridge.fit)
glmnet.ridge.fit$lambda
which.min(glmnet.ridge.fit$lambda)
which.max(glmnet.ridge.fit$lambda)
which.min(glmnet.ridge.fit$cvm)
cvm.min.index <- which.min(glmnet.ridge.fit$cvm)
cvm.min.index
plot(glmnet.ridge.fit$glmnet.fit,xvar = "lambda", label = TRUE)
abline(v = log(glmnet.ridge.fit$lambda[cvm.min.index]),lty = "dashed", col = "black" )
predict(glmnet.ridge.fit,X[test, ], type = "coefficients", s = "lambda.min")
glmnet.ridge.fit.predict <- predict(glmnet.ridge.fit,X[test, ], type = "coefficients", s = "lambda.min")
mean((glmnet.ridge.fit.predict - y[training])^2)
mean((glmnet.ridge.fit.predict - y[test])^2)
glmnet.lasso.fit <- cv.glmnet(X[training,],y[training],type.measure = "mse",family = "gaussian", alpha = 1, standardize = TRUE)
plot(glmnet.lasso.fit)
cvm.min.index.lasso <- which.min(glmnet.lasso.fit$cvm)
glmnet.lasso.fit.predict <- predict(glmnet.lasso.fit,X[test, ],type = "coefficients", s = "lambda.min")
glmnet.lasso.fit.predict
mean((glmnet.lasso.fit.predict - y[test])^2)
y[test]
y[test,]
y[test, ]
library(xlsx)
ans3.data <- read.xlsx(".data/ans3.xlsx",sheetIndex = 1,header = TRUE)
ans3 <- download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx",destfile = "./data/ans3.xlsx", method = "curl")
library(xlsx)
ans3.data <- read.xlsx(".data/ans3.xlsx",sheetIndex = 1,header = TRUE)
list.files("./data/")
ans3.data <- read.xlsx("./data/ans3.xlsx",sheetIndex = 1,header = TRUE)
ans3.data
head(ans3.data)
colIndex <- 7 : 15
rowIndex <- 18 : 23
ans3.data.subset <- read.xlsx("./data/ans3.xlsx",sheetIndex = 1,colIndex = colIndex,rowIndex = rowIndex)
ans3.data.subset
rm(ans3.data.subset)
dat <- read.xlsx("./data/ans3.xlsx",sheetIndex = 1,colIndex = colIndex,rowIndex = rowIndex)
dat
sum(dat$Zip*dat$Ext,na.rm=T)
library(XML)
ans4.data <- xmlTreeParse("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml",useInternal = TRUE)
ans4.file <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- xmlTreeParse(ans4.file,useInternal = TRUE)
ans4.data <- xmlTreeParse(ans4.file,useInternalNodes = TRUE)
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- xmlTreeParse(ans4.xml,useInternal = TRUE)
?xmlTreeParse
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- xmlTreeParse(ans4.xml,useInternal = TRUE)
file.xml <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(file.xml,useInternal = TRUE)
doc
ans4.data <- htmlTreeParse(ans4.xml,useInternal = TRUE)
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- htmlTreeParse(ans4.xml,useInternal = TRUE)
library(RCurl)
rm(Boston)
load(MASS)
library(MASS)
load(Boston)
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- getURL(ans4.xml)
ans4.data.doc <- xmlParse(ans4.data)
ans4.data.doc
ans4.rootNode <- xmlRoot(ans4.data.doc)
ans4.rootNode
xmlName(ans4.rootNode)
names(ans4.rootNode)
ans4.rootNode[[1]]
xmlApply(ans4.rootNode,xmlValue)
?xmlApply
xpathSApply(doc = ans4.rootNode,"//zipcode",xmlValue)
ans4.zip <- xpathSApply(doc = ans4.rootNode,"//zipcode",xmlValue)
ans4.zip
zip <- 0
for(i in ans4.zip){
if(i == 21231){
zip = zip +1
}
zip
}
ans5 <- download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv",destfile = "./data/ans5.csv",method = "curl")
ans5.data <- read.table("./data/ans5.csv",sep = ",", header = TRUE)
ans5.data
summary(ans5.data)
?fred
?fred()
?fread
?fread()
ans5.data$pwgtp15
mean(ans5.data$pwgtp15)
source("http://bioconductor.org/biocLite.R")
biocLite("hdf5")
library(hdf5)
biocLite("hdf5")
source("http://bioconductor.org/biocLite.R")
biocLite("hdf5")
source("http://bioconductor.org/biocLite.R")
biocLite("hdf5")
source("http://bioconductor.org/biocLite.R")
biocLite("hdf5")
htmlcode <- readLines(con)
con <- url("https://scholar.google.com/citations?user=HI-I6C0AAAAJ")
htmlcode <- readLines(con)
con <- url("https://scholar.google.com/citations?user=HI-I6C0AAAAJ")
htmlcode <- readLines(con)
con <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ@hl=en")
htmlcode <- readLines(con)
con <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlcode <- readLines(con)
con <- url("https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlcode <- readLines(con)
con <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlcode <- readLines(con)
con <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
close(con)
close(con)
close(con)
con <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlcode <- readLines(con)
htmlcode
close(con)
library(XML)
library(XML)
url <- con <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
url <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
html <- htmlTreeParse(url,useInternalNodes = TRUE)
url <- url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
html <- htmlTreeParse(url,useInternalNodes = TRUE)
ls()
rm(url)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url,useInternalNodes = TRUE)
xpathSApply(html,path = "//title",xmlValues)
xpathSApply(html,path = "//title",xmlValue)
xpathSApply(html,path = "//td[@id = 'col-citedby']",xmlValue)
xpathSApply(html,path = "//td[@id ='col-citedby']",xmlValue)
library(httr)
html2 <- get(url)
html2 <- GET(url)
content <- content(html2,as="text")
content
parsehtml <- htmlParse(content,asText = TRUE)
parsehtml
xpathSApply(parsehtml,path = "//title",xmlValue)
clientID <- "e70c82ebac4e43e5f133"
clientSecret <- "4b037333bfe29746ea8cf10923fa957fbe9baef0"
myapp <- oauth_app("newDataScientist",key = clientID,secret = clientSecret)
token <- "e11a48c2365e77c6e549a93f8ae90a7d79add7be"
token_1 <- "e11a48c2365e77c6e549a93f8ae90a7d79add7be"
sig <- sign_oauth1.0(myapp,token = token_1,token_secret = token_1)
homeGIT <- GET("https://api.github.com/users/jtleek/repos",sig)
homeGIT
contentGit <- content(homeGIT)
contentGit
tokenSecret <- "656ea15413c7267d944fa5c68d670e00b4b21a04"
tokenName <- "newDataScientist"
myapp <- oauth_app("newDataScientist",key = clientID,secret = clientSecret)
sig <- sign_oauth1.0(myapp,token = tokenName,token_secret = token_secret)
sig <- sign_oauth1.0(myapp,token = tokenName,token_secret = tokenSecret)
homeGIT <- GET("https://api.github.com/users/jtleek/repos",sig)
homeGIT
tokenName <- "e70c82ebac4e43e5f133"
tokenSecret <- "656ea15413c7267d944fa5c68d670e00b4b21a04"
#
myapp <- oauth_app("newDataScientist",key = clientID,secret = clientSecret)
sig <- sign_oauth1.0(myapp,token = tokenName,token_secret = tokenSecret)
homeGIT <- GET("https://api.github.com/users/jtleek/repos",sig)
homeGIT
rm(list = ls())
X <- data.frame("var1" = sample(1:5), "var2" = sample(6:10), "var3"= sample(11:15))
X
X <- X[sample(1:5),]
X
X
X <- X[sample(1:5),]
X <- X[sample(1:5),]
X
X$var2[c(1,3)] = NA
X
X[,1]
X[,"var1"]
X[1:2,"var2"]
X[(X$var3 >= 14 & X$var1 >= 4),]
X[(X$var3 >= 14 | X$var1 >= 4),]
X[which(X$var1 > 3),]
X[,1]
X[,which(X$var1 > 3)]
sort(X$var1)
sort(X$var3,ddecreasing = TRUE)
sort(X$var3,decreasing = TRUE)
sort(X$var3,decreasing = FALSE)
sort(X$var2,na.last = TRUE)
X[order(X$var1),]
X[order(X$var1,X$var3),]
X[order(X$var1,X$var3),]
library(plyr)
arrange(X,var1)
arrange(X,desc(var1)
arrange(X,desc(var1))
arrange(X,desc(var1))
Y <- cbind(X,rnorm(5))
Y
Yl <- cbind(rnorm(5), X)
Yl
Yr <- rbind(X,rnorm(5))
Yr
rm(list = ls())
getwd()
restData <- "Restaurants.csv"
restData <- read.csv("Restaurants.csv")
restData
View(restData)
head(restData,n = 4)
head(restData,n = 4)
head(restData,n = 3)
summary(restData)
str(restData)
quantile(restData$councilDistrict,na.rm = TRUE)
table(restData$zipCode,useNA = "ifany")
table(restData$councilDistrict,restData$zipCode)
sum(is.na(restData$councilDistrict))
sum(is.na(restData$zipCode))
any(is.na(restData$councilDistrict))
any(is.na(restData$policeDistrict())
any(is.na(restData$policeDistrict)
any(is.na(restData$policeDistrict)
any(is.na(restData$policeDistrict))
any(is.na(restData$policeDistrict))
colSums(is.na(restData))
all(colSums(is.na(restData)) == 0)
table(restData$zipCode %in% c("21212"))
table(restData$zipCode %in% c("21212", "21213"))
restData[table(restData$zipCode %in% c("21212", "21213")),]
restData[restData$zipCode %in% c("21212", "21213"),]
restData[restData$zipCode %in% c("21212", "21213"),] # only the rows with 21212 and 2123 zipcodes
object.size(restData)
print(object.size(restData),units = "Mb")
object.size(restData)
object.size(restData)
print(object.size(restData),units = "Mb")
restData$neighborhood %in% c("Roland Park", "Homaland")
restData$nearMe <- restData$neighborhood %in% c("Roland Park", "Homaland")
table(restData$nearMe)
restData$zipWrong <- ifelse(restData$zipCode < 0,TRUE,FALSE)
table(restData$zipWrong,restData$zipCode < 0)
?factor
restData$zcf <- factor(restData$zipCode)
restData$zcf(1:4)
restData$zcf[1:4]
class(restData$zcf)
yesno <- sample(c("yes","no"),size = 10,replace = TRUE)
yesno
yesnofec <- factor(restData$zcf,levels = c("yes","no"))
relevel(yesnofec,lef = "yes")
relevel(yesnofec,ref = "yes")
as.numeric(yesnofec)
yesnofec <- factor(yesno,levels = c("yes","no"))
relevel(yesnofec,ref = "yes")
as.numeric(yesnofec)
library(plyr)
library(Hmisc)
library(Hmisc)
restData2 <- mutate(restData,zipGroups = cut2(zipCode,g = 4))
table(restData2$zipGroups)
?table
