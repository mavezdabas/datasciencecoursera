myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,myURL)
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,myStopwords)
#making entra copy of the corpus
myCorpusCopy <- myCorpus
#stem World
# install.packages("SnowballC")
# library(SnowballC)
myCorpus <- tm_map(myCorpus,stemDocument)
#print first 5 lines from the corpus
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
myCorpus <- tm_map(myCorpus,stemCompletion,dictionary = myCorpusCopy,lazy = TRUE)
#count frequencies of minings and replacing that with minors
miningCases <- tm_map(myCorpusCopy,grep,pattern = "\\<mining")
sum(unlist(miningCases))
minerCases <- tm_map(myCorpusCopy,grep,pattern = "\\<miners")
sum(unlist(minerCases))
#replace miners with minings
myCorpus <- tm_map(myCorpus,gsub, pattern = "miners", replacement = "mining",lazy = TRUE)
tdm <- TermDocumentMatrix(myCorpus)
tdm <- TermDocumentMatrix(myCorpus,
control = list(wordLengths = c(1, Inf)))
library(SnowballC)
tweets <- userTimeline("RDataMining", n = 500)
length(tweets)
tweets[1:5]
#Text Cleaning
tweets.df <- twListToDF(tweets)
tweets.df
# library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
tdm <- TermDocumentMatrix(myCorpus,
control = list(removePunctuation = TRUE,
stopwords =  TRUE,
removeNumbers = TRUE, tolower = TRUE))
myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,myURL)
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,myStopwords)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus,stemDocument)
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
tdm
idx <- which(dimnames(tdm)$Terms == "r")
idx
inspect(tdm[idx + (0:5), 101:110])
(freq.terms <- findFreqTerms(tdm, lowfreq = 15))
term.freq <- rowSums(as.matrix(tdm))
term.freq
term.freq <- subset(term.freq, term.freq >= 15)
df <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
install.packages("ggplot2")
install.packages("ggplot2")
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
findAssocs(tdm, "r",0.2)
findAssocs(tdm, "m",0.2)
tdm
findAssocs(tdm, "a",0.2)
findAssocs(tdm, "mining", 0.25)
library(graph)
library(Rgraphviz)
install.packages("Rgraphviz")
install.packages("Rgraphviz")
install.packages("Rgraphviz")
install.packages("Rgraphviz")
library(Rgraphviz)
plot(tdm, term = freq.terms, corThreshold = 0.12, weighting = T)
install.packages("worldcloud")
install.packages("worldcloud")
install.packages("wordcloud")
library(wordcloud)
library(wordcloud)
m <- as.matrix(tdm)
m
word.freq <- sort(rowSums(m), decreasing = T)
word.freq
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,random.order = F)
tdm2 <- removeSparseTerms(tdm, sparse = 0.95) m2 <- as.matrix(tdm2)
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward")
fit <- hclust(distMatrix, method = "ward.D")
plot(fit)
rect.hclust(fit, k = 6)
m3 <- t(m2) # transpose the matrix to cluster documents (tweets)
set.seed(122) # set a fixed random seed
k <- 6 # number of clusters
kmeansResult <- kmeans(m3, k)
round(kmeansResult$centers, digits = 3) # cluster centers
for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep = ""))
s <- sort(kmeansResult$centers[i, ], decreasing = T) cat(names(s)[1:5], "\n")
# print the tweets of every cluster
for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep = ""))
s <- sort(kmeansResult$centers[i, ], decreasing = T)
cat(names(s)[1:5], "\n")
# print the tweets of every cluster
# print(tweets[which(kmeansResultÂ£cluster==i)])
}
library(flc)
library(fpc)
install.packages("fpc")
library(fpc)
# partitioning around medoids with estimation of number of clusters pamResult <- pamk(m3, metric="manhattan")
k <- pamResult$nc # number of clusters identified
pamResult <- pamk(m3, metric="manhattan")
k <- pamResult$nc
pamResult <- pamResult$pamobject
for (i in 1:k) {
cat("cluster", i, ": ", colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], "\n
}
)
)
stop
""
pamResult <- pamk(m3, metric="manhattan")
pamResult <- pamk(m3, metric="manhattan")
k <- pamResult$nc
pamResult <- pamResult$pamobject
for (i in 1:k) {
cat("cluster", i, ": ", colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], "\n"
}
for (i in 1:k) {
cat("cluster", i, ": ",
colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)]
}
for (i in 1:k) {
cat("cluster", i, ": ",
colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], "\n")
}
layout(matrix(c(1, 2), 1, 2)) # set to two graphs per page
plot(pamResult, col.p = pamResult$clustering)
layout(matrix(1))
plot(pamResult, col.p = pamResult$clustering)
dtm <- as.DocumentTermMatrix(tdm)
library(topicmodels)
install.packages("topicmodels")
install.packages("topicmodels")
library(topicmodels)
lda <- LDA(dtm, k = 8) # find 8 topics
term <- terms(lda, 4) # first 4 terms of every topic term
term
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
term
topic <- topics(lda, 1)
topics <- data.frame(date=as.IDate(tweets.df$created), topic)
topic <- data.frame(date=as.IDate(tweets.df$created), topic)
rm(list = ls())
directory.dummy <- "/Users/mavezsinghdabas/StatisticsForBigData-/eyemovement_data/BioEye2015_DevSets/RAN_30min_dv"
# Function to get all the files in the folder and removing all the unwanted
# data from the files.
completeFile <- function(directory,id = 1:306){
files_list <- list.files(directory,full.names = TRUE)
dat <- data.frame()
for( i in id){
dat <- rbind(dat,read.table(files_list[i],header = TRUE,fill = TRUE))
}
dat<- dat[c(-5,-6,-7,-8,-9,-10)]
colnames(dat) <- c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY")
dat
}
#To collobrate the whole data set togeather.
#This funciton takes time so for now we will work on a subset
#dataEye_1 <- completeFile(directory.dummy,id = 1:306)
#copydataEye_1 <- dataEye_1
#This is a subset of the with first 50 files
dataVersion_1 <- completeFile(directory.dummy,id = 1:50)
#Function defined as pixel_conversion_formula
#Input: Degree X and Y from the specified datasets
#Output: number of pixels away from the center of the display
#Function: pixel_conversion_formula<-function(data)
#Creating new column X.PIXEL and Y.PIXEL in the dataset
dataVersion_1$X.PIXEL <- pixel_conversion_formula(dataVersion_1$X.DEGREE)
dataVersion_1$Y.PIXEL <- pixel_conversion_formula(dataVersion_1$Y.DEGREE)
head(dataVersion_1)
library(CircStats)
#Define a formula to convert degrees of visual angle to pixels.
#This formula will return a value which is the number of pixels away
#from the center of the display, which will then need to be adjusted using the second formula in this script.
pixel_conversion_formula<-function(data){
#Define variables needed for the calculation:
d=55 #Distance between viewers and monitor in cm. Don't modify this.
h=297 #Monitor height in cm. Don't modify this.
r=1050 # Vertical resolution of the monitor. Don't modify this.
#Calculate the number of pixels per degree of visual angle:
deg_per_pix<-deg(atan2(.5*h,d)/(.5*r))
#Calculate the size of data object in pixels instead of degrees
size_in_pix<-data/deg_per_pix
return(size_in_pix)
}
#Takes pairs of x & y pixel values and shifts their origins to the upper
#left hand corner of the monitor. Works by adding a constant to the pixel coordinate value for each dimension that re-centers them with respect to the upper-left corner of the display treated as the origin (0,0).
convert_to_origin_formula_x <- function(x){
original_horizontal_coordinate<-840 # The value of the pixel at the horizontal locaiton of the original origin for this data. Don't modify this.
con_hori_crd<-x+original_horizontal_coordinate #Adds the re-centering adjustment to the horizontal value in the input tuple.
return(con_hori_crd) # Generate a tuple of converted coordinate values.
}
convert_to_origin_formula_y <- function(y){
original_vertical_coordinate<-525 # The value of the pixel at the vertical location of the original origin for this data. Don't modify this.
con_verti_crd<-y+original_vertical_coordinate #Adds the re-centering adjustment to the vertical value in the input tuple.
return(con_verti_crd) # Generate a tuple of converted coordinate values.
}
dataVersion_1$X.PIXEL <- pixel_conversion_formula(dataVersion_1$X.DEGREE)
dataVersion_1$Y.PIXEL <- pixel_conversion_formula(dataVersion_1$Y.DEGREE)
head(dataVersion_1)
#Function defined as convert_to_origin_formula
#Input: Takes pairs of x & y pixel values from the dataset which we
#calculated using the function 1
#Output: Shifts their origins to the upper left hand corner of the
#monitor. Works by adding a constant to the pixel coordinate
#value for each dimension that re-centers them with respect to the
#upper-left corner of the display treated as the origin (0,0).
dataVersion_1$X_original <- convert_to_origin_formula_x(dataVersion_1$X.PIXEL)
dataVersion_1$Y_original <- convert_to_origin_formula_y(dataVersion_1$Y.PIXEL)
head(dataVersion_1)
dataVersion_1 <- completeFile(directory.dummy,id = 1:306)
dataVersion_1$X.PIXEL <- pixel_conversion_formula(dataVersion_1$X.DEGREE)
dataVersion_1$Y.PIXEL <- pixel_conversion_formula(dataVersion_1$Y.DEGREE)
head(dataVersion_1)
tail(dataVersion_1)
dataVersion_1$X_original <- convert_to_origin_formula_x(dataVersion_1$X.PIXEL)
dataVersion_1$Y_original <- convert_to_origin_formula_y(dataVersion_1$Y.PIXEL)
head(dataVersion_1)
(0.3 * 0) / (0 * 0.2)
pchisq(q = (.95))
install.packages("aod")
library(aod)
wald.test(1-(.95/2))
pchisq(q = (.95),df = 1)
sqrt((pchisq(q = (.95),df = 1)))
qchisq(p = 0.05,df = 1)
qchisq(p = 0.95,df = 1)
setwd("/Users/ovitek/Dropbox/Olga/Teaching/Stat526/Spring14/LectureNotes/5-logistic")
X <- read.table("smokingAndObesity_1.txt", sep=" ", as.is=TRUE, header=TRUE)
1 - (.95)
1 - (0.05/2)
pchisq(q = .975)
pchisq(q = .975,df = 1)
sqrt(pchisq(q = .975,df = 1))
sqrt(pchisq(q = .975,df = 2))
1 - (0.05/2*2)
sqrt(pchisq(q = .95,df = 1))
sqrt(pchisq(q = .95,df = 2))
m <- c(8, 3, 7, 2, 8, 4, 7, 2,8, 5, 3, 7, 2,8, 6, 4, 7, 2 ,8, 6, 3, 4, 7, 2)
m
unique(m)
unique.array(m)
order(unique(m))
m1 <- c(8, 3, 7, 2)
m2 <- c(8, 4, 7, 2)
m3 <- c(8, 5, 3, 7, 2)
m4 <- c(8, 6, 4, 7, 2)
m5 <- c(8, 6, 3, 4, 7, 2)
unique(m1,m2,m3,m4,m5)
rm(list = ls())
78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
90
marks <- (78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
90)
marks <- (marks / 100)*40
marks <- (78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
90)
marksWeight <- (marks / 1000)*40
53.333 - marksWeight
50 + marksWeight
52 + marksWeight
54 + marksWeight
53 + marksWeight
88.889 - 53 + marksWeight
88.889 - (53 + marksWeight)
45 + marksWeight
47 + marksWeight
49 + marksWeight
48 + marksWeight
83.333 - 83.19304
49 + marksWeight
marks <- (78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
83)
marksWeight <- (marks / 1000)*40
# Final
49 + marksWeight #50 in final
1- .95
1 - (.05/2)
test1 <-  c(0.0000000, 0.0000000, 0.3333333, 0.3333333 ,0.3333333)
test1
test2 <- matrix(c(0.5, 0.5, 0.3333333,0.3333333, 0.1666667 ,0.1666667 ,0,0,0,0) , nrow = 2, ncol = 5 )
test2
test2 <- matrix(c(1, 2, 3,4, 5 ,6,0,0,0,0) , nrow = 2, ncol = 5 )
test2
test1 <-  c(0.0000000, 0.0000000, 0.3333333, 0.3333333 ,0.3333333)
test1
test2 <- matrix(c(0.5, 0.5, 0.3333333,0.3333333, 0.1666667 ,0.1666667 ,0,0,0,0) , nrow = 2, ncol = 5 )
test2
test2*test1
marks <- (78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
83)
marksWeight <- (marks / 1000)*40
49 + marksWeight #50 in final
49 + marksWeight #50 in final
marksWeight
48 + marksWeight #50 in final
H1 <- 73.50 #75
H2 <- 88    #100
H3 <- 110   #125
H4 <- 85    #85
H5 <- 125   #125
H6 <- 125   #125
H7 <- 50    #50
H8 <- 50    #50
H9 <- 100   #100
HT <- H1 + H2 + H3 + H4 + H5 + H6 + H7 + H8 + H9
HomeTotal <- 100 + 50 + 50 + 125 + 125  + 85 + 125 + 100 + 75
HomeworkWeight <- (HT/HomeTotal) * 100
HomeworkWeight
# 100
Midterm1 <- 61.50  #100
# 100
Midterm2 <- 79     #100
# 100
Project <- 100     #100
# 100
Final <- 90     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
Midterm2 <- 76     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
Midterm2 <- 76     #100
# 100
Project <- 100     #100
# 100
Final <- 85     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
Midterm2 <- 79     #100
(Total / 500) * 100
(Total / 500) * 100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
Final <- 80     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
Midterm1 <- 61.50  #100
# 100
Midterm2 <- 79     #100
# 100
Project <- 100     #100
# 100
Final <- 80     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
Midterm2 <- 76     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
Midterm2 <- 79     #100
# 100
Project <- 100     #100
# 100
Final <- 80     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
marks <- (78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
90)
marksWeight <- (marks / 1000)*40
marksWeight
51 + marksWeight #50 in final
55 + marksWeight #50 in final
54 + marksWeight #50 in final
53 + marksWeight #50 in final
marks <- (78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
93)
marksWeight <- (marks / 1000)*40
marksWeight
53 + marksWeight #50 in final
marks <- (78.892+
86.403	+
84.908	 +
89.583	+
89.104	+
87.765	+
93.382	 +
91.112	+
88.677+
94)
marksWeight <- (marks / 1000)*40
marksWeight
# Final
53 + marksWeight #50 in final
54 + marksWeight #50 in final
(Total / 500) * 100
H1 <- 73.50 #75
H2 <- 88    #100
H3 <- 110   #125
H4 <- 85    #85
H5 <- 125   #125
H6 <- 125   #125
H7 <- 50    #50
H8 <- 50    #50
H9 <- 100   #100
HT <- H1 + H2 + H3 + H4 + H5 + H6 + H7 + H8 + H9
HomeTotal <- 100 + 50 + 50 + 125 + 125  + 85 + 125 + 100 + 75
HomeworkWeight <- (HT/HomeTotal) * 100
HomeworkWeight
# 100
Midterm1 <- 61.50  #100
# 100
Midterm2 <- 79     #100
# 100
Project <- 100     #100
# 100
Final <- 80     #100
Total <- HomeworkWeight + Midterm1 + Midterm2 + Project + Final
(Total / 500) * 100
rm(list = ls())
plot(summary(rcc.cv.opls))
source("http://bioconductor.org/biocLite.R")
biocLite("Cardinal")
library(Cardinal)
source("http://bioconductor.org/biocLite.R")
biocLite("CardinalWorkflows")
library(CardinalWorkflows)
library("CardinalWorkflows")
data(rcc, rcc_analyses)
rcc.norm <- normalize(rcc,method = "tic")
summary(rcc.norm)
rcc.resample <- reduceDimension(rcc.norm,methods = "resample")
summary(rcc.resample)
summary(rcc$diagnosis)
rcc.small <- rcc.resample[,rcc$diagnosis %in% c("cancer","normal")]
summary(rcc.small)
rcc.pca <- PCA(rcc.small,ncomp = 5)
summary(rcc.pca)
pca.normal <- as.data.frame(rcc.pca[[1]]$scores[rcc.small$diagnosis == "normal",])
pca.cancer <- as.data.frame(rcc.pca[[1]]$scores[rcc.small$diagnosis == "cancer",])
summary(rcc.small$sample)
rcc.cv.pls <- cvApply(rcc.small, .y = rcc.small$diagnosis, .fun = "PLS", ncomp = 1:15)
plot(summary(rcc.cv.pls))
summary(rcc.cv.pls)$accuracy[["ncomp = 13"]]
rcc.pls <- PLS(rcc.small,y = rcc.small$diagnosis,ncomp = 13)
plot(rcc.pls,layout= c(1,1))
rcc.cv.opls <- cvApply(rcc.small, .y = rcc.small$diagnosis, .fun = "OPLS", ncomp = 1:15,keep.Xnew = FALSE)
plot(summary(rcc.cv.opls))
summary(rcc.cv.pls)$accuracy[["ncomp = 5"]]
setwd("~/datasciencecoursera/EXDAnalysis")
