find_rtools()
library(devtools)
library(devtools);
find.package("devtools")
find.package("devtools");
install.packages("devtools")
library(devtools)
find_rtools()
install.package("KernSmooth")
install.packages("KernSmooth")
import(KernSmooth)
imports(KernSmooth)
suppressMessages(require(KernSmooth))
import(KernSmooth)
add <- function(x,y) {
x + y
}
add(3,5)
above <- function(x,n) {
use <- x > n
x[use]
}
x <- 1:20
above(2)
above(x)
above(x,12)
above <- function(x,n = 10) {
use <- x > n
x[use]
}
above(x)
columnmean <- function(x) {
nc <- ncol(x)
means <- numeric(nc)
for(i in 1:nc) {
means[i] <- mean(x[,i])
}
means
}
columnmean(airquality)
columnmean <- function(x,removeNA = TRUE) {
nc <- ncol(x)
means <- numeric(nc)
for(i in 1:nc) {
means[i] <- mean(x[,i], na.rm = removeNA)
}
means
}
columnmean(airquality)
?conf.int
?confint
confint(data4.lm,level = .98)
confint(data4.lm,level = .98)
E
EEEEE
EE
E
v v v v v
n n n n nb n nb nb n
b bn nb n n n bhjb jh <- njb
bbbbhbhbhbhbbbbbbbbbbbb
vgvhv
bhhbhbhbhbhbh
hbkh
j j j j bbbbbb
njnj
hjh
require(igraph)
library(Rgraphviz)
source("http://www.bioconductor.org/biocLite.R")
biocLite("Ruuid")
biocLite("graph")Replace broken link
biocLite("graph")
biocLite("Rgraphviz")
library(Rgraphviz)
string <- HABBIEIHIHAAHAEBGCHBBHICDHGCHIHBBGBBBBGGIGACHABHACIBCBGIIBBAIBAIABBBAIGEBBCBCBCBDACBBBBFAAGAAAAAFAFF
string <- "HABBIEIHIHAAHAEBGCHBBHICDHGCHIHBBGBBBBGGIGACHABHACIBCBGIIBBAIBAIABBBAIGEBBCBCBCBDACBBBBFAAGAAAAAFAFF"
?str_count()
library(stringr)
?str_count()
str_count(string,'A')
str_count(string,'B')
str_count(string,'C')
str_count(string,'D')
str_count(string,'E')
str_count(string,'F')
str_count(string,'G')
str_count(string,'H')
str_count(string,'I')
str_count(string,'J')
str_count(string,'K')
str_count(string,'L')
str_count(string,'M')
str_count(string,'N')
str_count(string,'O')
str_count(string,'P')
str_count(string,'Q')
str_count(string,'R')
str_count(string,'S')
str_count(string,'T')
str_count(string,'U')
str_count(string,'V')
str_count(string,'W')
str_count(string,'X')
str_count(string,'Y')
str_count(string,'Z')
str_count(string)
count(string)
str_count(string,)
str_count(string, ' ')
n <- 011110011100111110101110011011011111110
n
n <- c(011110011100111110101110011011011111110 )
n
library(bestglm)
install.packages(bestglm)
install.packages("bestglm")
library(bestglm)
library(Boston)
library(MASS)
Boston
set.seed(1)
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
training
summary(Boston)
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
training
training
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xybind <- cbind(X,y)
Xybind
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston <- bestglm(Xybind,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston <- bestglm(Xybind,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
training <- sample(c(TRUE,FALSE), rnorm(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
training
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
?Boston
?bestglm
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 3, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
set.seed(1)
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 6, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
training <- sample(c(TRUE,FALSE), nrow(Boston),rep = TRUE)
test <- (!training)
X <- Boston[training, -14]
y <- Boston[training, 14]
Xy <- cbind(X,y)
bestglm.boston <- bestglm(Xy,family = gaussian, IC = "CV",CVArgs = list(Method = "d", K = 10, REP = 1),TopModels = 5, method = "exhaustive" ,nvmax = "default")
bestglm.boston$Title
bestglm.boston$ModelReport
bestglm.boston$BestModel
library(glmnet)
X <- model.matrix(crim ~ ., Boston)
X
X <- model.matrix(crim ~ ., Boston)[,-1]
X
y <- Boston$crim
y
X <- model.matrix(crim ~ ., Boston)[,-1]
y <- Boston$crim
X <- model.matrix(crim ~ ., Boston)[,-1]
y <- Boston$crim
set.seed(1)
nfolds <- nrow(Boston)
glmnet.ridge.fit <- cv.glmnet(X[training,],y[training],type.measure = "mse",family = "gaussian", alpha = 0, standardize = TRUE)
glmnet.ridge.fit
plot(glmnet.ridge.fit)
glmnet.ridge.fit$lambda
which.min(glmnet.ridge.fit$lambda)
which.max(glmnet.ridge.fit$lambda)
which.min(glmnet.ridge.fit$cvm)
cvm.min.index <- which.min(glmnet.ridge.fit$cvm)
cvm.min.index
plot(glmnet.ridge.fit$glmnet.fit,xvar = "lambda", label = TRUE)
abline(v = log(glmnet.ridge.fit$lambda[cvm.min.index]),lty = "dashed", col = "black" )
predict(glmnet.ridge.fit,X[test, ], type = "coefficients", s = "lambda.min")
glmnet.ridge.fit.predict <- predict(glmnet.ridge.fit,X[test, ], type = "coefficients", s = "lambda.min")
mean((glmnet.ridge.fit.predict - y[training])^2)
mean((glmnet.ridge.fit.predict - y[test])^2)
glmnet.lasso.fit <- cv.glmnet(X[training,],y[training],type.measure = "mse",family = "gaussian", alpha = 1, standardize = TRUE)
plot(glmnet.lasso.fit)
cvm.min.index.lasso <- which.min(glmnet.lasso.fit$cvm)
glmnet.lasso.fit.predict <- predict(glmnet.lasso.fit,X[test, ],type = "coefficients", s = "lambda.min")
glmnet.lasso.fit.predict
mean((glmnet.lasso.fit.predict - y[test])^2)
y[test]
y[test,]
y[test, ]
library(xlsx)
ans3.data <- read.xlsx(".data/ans3.xlsx",sheetIndex = 1,header = TRUE)
ans3 <- download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx",destfile = "./data/ans3.xlsx", method = "curl")
library(xlsx)
ans3.data <- read.xlsx(".data/ans3.xlsx",sheetIndex = 1,header = TRUE)
list.files("./data/")
ans3.data <- read.xlsx("./data/ans3.xlsx",sheetIndex = 1,header = TRUE)
ans3.data
head(ans3.data)
colIndex <- 7 : 15
rowIndex <- 18 : 23
ans3.data.subset <- read.xlsx("./data/ans3.xlsx",sheetIndex = 1,colIndex = colIndex,rowIndex = rowIndex)
ans3.data.subset
rm(ans3.data.subset)
dat <- read.xlsx("./data/ans3.xlsx",sheetIndex = 1,colIndex = colIndex,rowIndex = rowIndex)
dat
sum(dat$Zip*dat$Ext,na.rm=T)
library(XML)
ans4.data <- xmlTreeParse("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml",useInternal = TRUE)
ans4.file <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- xmlTreeParse(ans4.file,useInternal = TRUE)
ans4.data <- xmlTreeParse(ans4.file,useInternalNodes = TRUE)
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- xmlTreeParse(ans4.xml,useInternal = TRUE)
?xmlTreeParse
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- xmlTreeParse(ans4.xml,useInternal = TRUE)
file.xml <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(file.xml,useInternal = TRUE)
doc
ans4.data <- htmlTreeParse(ans4.xml,useInternal = TRUE)
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- htmlTreeParse(ans4.xml,useInternal = TRUE)
library(RCurl)
rm(Boston)
load(MASS)
library(MASS)
load(Boston)
ans4.xml <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
ans4.data <- getURL(ans4.xml)
ans4.data.doc <- xmlParse(ans4.data)
ans4.data.doc
ans4.rootNode <- xmlRoot(ans4.data.doc)
ans4.rootNode
xmlName(ans4.rootNode)
names(ans4.rootNode)
ans4.rootNode[[1]]
xmlApply(ans4.rootNode,xmlValue)
?xmlApply
xpathSApply(doc = ans4.rootNode,"//zipcode",xmlValue)
ans4.zip <- xpathSApply(doc = ans4.rootNode,"//zipcode",xmlValue)
ans4.zip
zip <- 0
for(i in ans4.zip){
if(i == 21231){
zip = zip +1
}
zip
}
ans5 <- download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv",destfile = "./data/ans5.csv",method = "curl")
ans5.data <- read.table("./data/ans5.csv",sep = ",", header = TRUE)
ans5.data
summary(ans5.data)
?fred
?fred()
?fread
?fread()
ans5.data$pwgtp15
mean(ans5.data$pwgtp15)
